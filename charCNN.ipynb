{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CharCNN\n",
    "\n",
    "Code source: https://github.com/L1aoXingyu/Char-RNN-Gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jingyunyang/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import codecs\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from mxnet import gluon as g\n",
    "\n",
    "\n",
    "class TextConverter(object):\n",
    "    def __init__(self, text_path, max_vocab=5000):\n",
    "        ## Read in data(abstract):\n",
    "        with codecs.open(text_path, mode='r', encoding='utf-8') as f:\n",
    "            text_file = f.readlines()\n",
    "        word_list = [v for s in text_file for v in s]\n",
    "        vocab = set(word_list)\n",
    "        # get word frequency\n",
    "        vocab_count = {}\n",
    "        for word in vocab:\n",
    "            vocab_count[word] = 0\n",
    "        for word in word_list:\n",
    "            vocab_count[word] += 1\n",
    "        vocab_count_list = []\n",
    "        for word in vocab_count:\n",
    "            vocab_count_list.append((word, vocab_count[word]))\n",
    "        vocab_count_list.sort(key=lambda x: x[1], reverse=True)\n",
    "        # if exceeds max word length, delete words with least word frequency \n",
    "        if len(vocab_count_list) > max_vocab:\n",
    "            vocab_count_list = vocab_count_list[:max_vocab]\n",
    "        vocab = [x[0] for x in vocab_count_list]\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.word_to_int_table = {c: i for i, c in enumerate(self.vocab)}\n",
    "        self.int_to_word_table = dict(enumerate(self.vocab))\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab) + 1\n",
    "\n",
    "    def word_to_int(self, word):\n",
    "        if word in self.word_to_int_table:\n",
    "            return self.word_to_int_table[word]\n",
    "        else:\n",
    "            return len(self.vocab)\n",
    "\n",
    "    def int_to_word(self, index):\n",
    "        if index == len(self.vocab):\n",
    "            return '<unk>'\n",
    "        elif index < len(self.vocab):\n",
    "            return self.int_to_word_table[index]\n",
    "        else:\n",
    "            raise Exception('Unknow index!')\n",
    "\n",
    "    def text_to_arr(self, text):\n",
    "        arr = []\n",
    "        for word in text:\n",
    "            arr.append(self.word_to_int(word))\n",
    "        return np.array(arr)\n",
    "\n",
    "    def arr_to_text(self, arr):\n",
    "        words = []\n",
    "        for index in arr:\n",
    "            words.append(self.int_to_word(index))\n",
    "        return \"\".join(words)\n",
    "\n",
    "\n",
    "class TextData(g.data.Dataset):\n",
    "    def __init__(self, text_path, n_step, arr_to_idx):\n",
    "        self.n_step = n_step\n",
    "\n",
    "        with codecs.open(text_path, mode='r', encoding='utf-8') as f:\n",
    "            data = f.readlines()\n",
    "        text = [v for s in data for v in s]\n",
    "        num_seq = int(len(text) / n_step)\n",
    "        self.num_seq = num_seq\n",
    "        text = text[:num_seq * n_step]  # cut th\n",
    "        arr = arr_to_idx(text)\n",
    "        arr = arr.reshape((num_seq, -1))\n",
    "        self.arr = arr\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.arr[index, :]\n",
    "        y = np.zeros(x.shape)\n",
    "        y[:-1], y[-1] = x[1:], x[0]\n",
    "        return nd.array(x), nd.array(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from mxnet import gluon as g\n",
    "\n",
    "\n",
    "class CharRNN(g.Block):\n",
    "    def __init__(self, num_classes, embed_dim, hidden_size, num_layers,\n",
    "                 dropout):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.word_to_vec = g.nn.Embedding(num_classes, embed_dim)\n",
    "            self.rnn = g.rnn.GRU(hidden_size, num_layers, dropout=dropout)\n",
    "            self.proj = g.nn.Dense(num_classes)\n",
    "\n",
    "    def forward(self, x, hs=None):\n",
    "        batch = x.shape[0]\n",
    "        if hs is None:\n",
    "            hs = nd.zeros(\n",
    "                (self.num_layers, batch, self.hidden_size), ctx=mx.gpu())\n",
    "        word_embed = self.word_to_vec(x)  # batch x len x embed\n",
    "        word_embed = word_embed.transpose((1, 0, 2))  # len x batch x embed\n",
    "        out, h0 = self.rnn(word_embed, hs)  # len x batch x hidden\n",
    "        le, mb, hd = out.shape\n",
    "        out = out.reshape((le * mb, hd))\n",
    "        out = self.proj(out)\n",
    "        out = out.reshape((le, mb, -1))\n",
    "        out = out.transpose((1, 0, 2))  # batch x len x hidden\n",
    "        return out.reshape((-1, out.shape[2])), h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unicode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ecd8477bb88d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-ecd8477bb88d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--clip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'权重上限'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--use-gpu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'是否使用的GPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1728\u001b[0m     \u001b[0;31m# =====================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1730\u001b[0;31m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_known_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1731\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unrecognized arguments: %s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36mparse_known_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1760\u001b[0m         \u001b[0;31m# parse the arguments and exit if there are any errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1762\u001b[0;31m             \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_known_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_UNRECOGNIZED_ARGS_ATTR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_UNRECOGNIZED_ARGS_ATTR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36m_parse_known_args\u001b[0;34m(self, arg_strings, namespace)\u001b[0m\n\u001b[1;32m   1991\u001b[0m                         action.default is getattr(namespace, action.dest)):\n\u001b[1;32m   1992\u001b[0m                         setattr(namespace, action.dest,\n\u001b[0;32m-> 1993\u001b[0;31m                                 self._get_value(action, action.default))\n\u001b[0m\u001b[1;32m   1994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1995\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequired_actions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, action, arg_string)\u001b[0m\n\u001b[1;32m   2288\u001b[0m         \u001b[0;31m# convert the value to the appropriate type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2289\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2290\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2292\u001b[0m         \u001b[0;31m# ArgumentTypeErrors indicate errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-ecd8477bb88d>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    105\u001b[0m     parser.add_argument(\n\u001b[1;32m    106\u001b[0m         '--dropout', default=0.5, type=float, help='RNN中drop的概率')\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'我'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'给出生成文本的开始'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--pred_len'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'生成文本的长度'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--checkpoint'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'载入模型的位置'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unicode' is not defined"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from mxnet import gluon as g\n",
    "\n",
    "\n",
    "\n",
    "def train_epoch(ctx, model, dataloader, criterion, optimizer, clip):\n",
    "    running_loss = 0.0\n",
    "    n_total = 0.0\n",
    "    for batch in dataloader:\n",
    "        x, y = batch\n",
    "        y = y.astype('float32').as_in_context(ctx).swapaxes(0, 1)\n",
    "        x = x.as_in_context(ctx).swapaxes(0, 1)\n",
    "        mb_size = x.shape[0]\n",
    "        with g.autograd.record():\n",
    "            out = model(x)\n",
    "            batch_loss = criterion(out, y)\n",
    "        batch_loss.backward()\n",
    "\n",
    "        grads = [i.grad(ctx) for i in model.collect_params().values()]\n",
    "        total_norm = g.utils.clip_global_norm(grads, clip * y.shape[0] * y.shape[1])\n",
    "\n",
    "        if np.isfinite(total_norm):\n",
    "            optimizer.step(mb_size)\n",
    "            running_loss += nd.sum(batch_loss).asscalar()\n",
    "            n_total += mb_size\n",
    "        else:\n",
    "            raise UserWarning('nan/inf detected. skipping batch')\n",
    "    return running_loss / n_total\n",
    "\n",
    "\n",
    "def train(ctx, n_epoch, model, dataloader, optimizer, criterion, clip):\n",
    "    for e in range(n_epoch):\n",
    "        print('{}/{}'.format(e + 1, n_epoch))\n",
    "        since = time.time()\n",
    "        loss = train_epoch(ctx, model, dataloader, criterion, optimizer, clip)\n",
    "        print('Loss: {:.6f}, Time: {:.3} s'.format(loss, time.time() - since))\n",
    "        if (e + 1) % 1000 == 0:\n",
    "            if not os.path.exists('./checkpoints'):\n",
    "                os.mkdir('./checkpoints')\n",
    "            model.save_params('./checkpoints/model_{}.params'.format(e + 1))\n",
    "\n",
    "\n",
    "def pick_top_n(preds, top_n=5):\n",
    "    top_pred_prob, top_pred_label = nd.topk(preds, axis=2, k=top_n, ret_typ='both')\n",
    "    top_pred_label = top_pred_label.asnumpy()\n",
    "    top_pred_prob /= nd.sum(top_pred_prob, axis=2, keepdims=True)\n",
    "    top_pred_prob = top_pred_prob.asnumpy().reshape((-1, ))\n",
    "    top_pred_label = top_pred_label.reshape((-1, ))\n",
    "    c = np.random.choice(top_pred_label, size=1, p=top_pred_prob)\n",
    "    return c\n",
    "\n",
    "\n",
    "def sample(ctx, model, checkpoint, convert, arr_to_text, prime, text_len=20):\n",
    "    '''\n",
    "    将载入好权重的模型读入，指定开始字符和长度进行生成，将生成的结果保存到txt文件中\n",
    "    checkpoint: 载入的模型\n",
    "    convert: 文本和下标转换\n",
    "    prime: 起始文本\n",
    "    text_len: 生成文本长度\n",
    "    '''\n",
    "    model.load_params(checkpoint, ctx=ctx)\n",
    "    samples = [convert(c) for c in prime]\n",
    "    input_txt = nd.array(samples).reshape((-1 ,1)).as_in_context(ctx)\n",
    "    embed = model[0](input_txt)\n",
    "    hs = nd.zeros(model[1].state_info(1)[0]['shape'], ctx=ctx)\n",
    "    _, init_state = model[1](embed, hs)\n",
    "\n",
    "    result = samples\n",
    "    model_input = input_txt[:, input_txt.shape[1] - 1].reshape((-1, 1))\n",
    "    for i in range(text_len):\n",
    "        # out是输出的字符，大小为1 x vocab\n",
    "        # init_state是RNN传递的hidden state\n",
    "        with mx.autograd.predict_mode():\n",
    "            embed = model[0](model_input)\n",
    "            out, init_state = model[1](embed, init_state)\n",
    "            out = model[2](out)\n",
    "        pred = pick_top_n(out)\n",
    "        model_input = nd.array(pred).reshape((-1, 1)).as_in_context(ctx)\n",
    "        result.append(pred[0])\n",
    "    return arr_to_text(result)\n",
    "\n",
    "\n",
    "def main():\n",
    "    '''main function'''\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--state', required=True, help='训练还是预测, train or eval')\n",
    "    parser.add_argument('--txt', required=True, help='进行训练的txt文件')\n",
    "    parser.add_argument('--batch', default=128, type=int, help='训练的batch size')\n",
    "    parser.add_argument('--epoch', default=5000, type=int, help='跑多少个epoch')\n",
    "    parser.add_argument('--len', default=100, type=int, help='输入模型的序列长度')\n",
    "    parser.add_argument(\n",
    "        '--max_vocab', default=5000, type=int, help='最多存储的字符数目')\n",
    "    parser.add_argument('--embed', default=512, type=int, help='词向量的维度')\n",
    "    parser.add_argument('--hidden', default=512, type=int, help='RNN的输出维度')\n",
    "    parser.add_argument('--n_layer', default=2, type=int, help='RNN的层数')\n",
    "    parser.add_argument(\n",
    "        '--dropout', default=0.5, type=float, help='RNN中drop的概率')\n",
    "    parser.add_argument('--begin', default='我', type=lambda s: unicode(s, 'utf8'), help='给出生成文本的开始')\n",
    "    parser.add_argument('--pred_len', default=20, type=int, help='生成文本的长度')\n",
    "    parser.add_argument('--checkpoint', help='载入模型的位置')\n",
    "    parser.add_argument('--clip', default=0.2, type=float, help='权重上限')\n",
    "    parser.add_argument('--use-gpu', default=True, help='是否使用的GPU')\n",
    "    opt = parser.parse_args()\n",
    "    print(opt)\n",
    "\n",
    "    convert = TextConverter(opt.txt, max_vocab=opt.max_vocab)\n",
    "    model = g.nn.Sequential()\n",
    "    with model.name_scope():\n",
    "        model.add(g.nn.Embedding(convert.vocab_size, opt.embed))\n",
    "        model.add(g.rnn.GRU(opt.hidden, opt.n_layer, dropout=opt.dropout))\n",
    "        model.add(g.nn.Dense(convert.vocab_size, flatten=False))\n",
    "\n",
    "    ctx = mx.gpu(0) if opt.use_gpu else mx.cpu()\n",
    "    model.initialize(ctx=ctx)\n",
    "\n",
    "    if opt.state == 'train':\n",
    "        dataset = TextData(opt.txt, opt.len, convert.text_to_arr)\n",
    "        dataloader = g.data.DataLoader(dataset, opt.batch, shuffle=True)\n",
    "        lr_sch = mx.lr_scheduler.FactorScheduler(\n",
    "            int(1000 * len(dataloader)), factor=0.1)\n",
    "        optimizer = g.Trainer(model.collect_params(), 'adam', {\n",
    "            'learning_rate': 1e-3,\n",
    "            'clip_gradient': 3,\n",
    "            'lr_scheduler': lr_sch\n",
    "        })\n",
    "        cross_entropy = g.loss.SoftmaxCrossEntropyLoss()\n",
    "        train(ctx, opt.epoch, model, dataloader, optimizer, cross_entropy, opt.clip)\n",
    "\n",
    "    elif opt.state == 'eval':\n",
    "        pred_text = sample(ctx, model, opt.checkpoint, convert.word_to_int,\n",
    "                           convert.arr_to_text, opt.begin, opt.pred_len)\n",
    "        print(pred_text)\n",
    "        with open('./generate.txt', 'a') as f:\n",
    "            f.write(pred_text)\n",
    "            f.write('\\n')\n",
    "    else:\n",
    "        print('Error state, must choose from train and eval!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
