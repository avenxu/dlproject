{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers of titles = 51649\n",
      "\tMax number of tokens in a sentence = 37\n",
      "\tMin number of tokens in a sentence = 1\n",
      "Number of papers of abstract = 51649\n",
      "\tMax number of tokens in a sentence = 574\n",
      "\tMin number of tokens in a sentence = 2\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "data = []\n",
    "for file_,label in zip([\"titles.txt\",\"abstract.txt\"],[0,1]):\n",
    "    lines = open(file_).readlines()\n",
    "    lines = list(map(lambda x:x.strip().replace(\"-\",\" \").split(),lines))\n",
    "    for line in lines:\n",
    "        data.append([line,label])\n",
    "    print(\"Number of papers of {} = {}\".format(file_[:-4],len(lines)))\n",
    "    print(\"\\tMax number of tokens in a sentence = {}\".format(max(map(lambda x:len(x),lines))))\n",
    "    print(\"\\tMin number of tokens in a sentence = {}\".format(min(map(lambda x:len(x),lines))))\n",
    "random.Random(5).shuffle(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguity and noise in natural language instructions create a significant barrier towards adopting autonomous systems into safety critical workflows involving humans and machines. In this paper, we propose to build on recent advances in electrophysiological monitoring methods and augmented reality technologies, to develop alternative modes of communication between humans and robots involved in large scale proximal collaborative tasks. We will first introduce augmented reality techniques for projecting a robot's intentions to its human teammate, who can interact with these cues to engage in real time collaborative plan execution with the robot. We will then look at how electroencephalographic (EEG) feedback can be used to monitor human response to both discrete events, as well as longer term affective states while execution of a plan. These signals can be used by a learning agent, a.k.a an affective robot, to modify its policy. We will present an end to end system capable of demonstrating these modalities of interaction. We hope that the proposed system will inspire research in augmenting human robot interactions with alternative forms of communications in the interests of safety, productivity, and fluency of teaming, particularly in engineered settings such as the factory floor or the assembly line in the manufacturing industry where the use of such wearables can be enforced.\n"
     ]
    }
   ],
   "source": [
    "# See some randomly sampled sentences\n",
    "print(\" \".join(data[random.randint(0,len(data))][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
