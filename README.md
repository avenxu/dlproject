## Text Summarization and Generation on Arxiv Paper Abstracts

#### Project Description
This goal is to train an LSTM on the Abstract and map the paper title to the abstract using seq2seq from a similar idea of machine translation.  
There are two ways of doing this. Can either input keywords and title to predict the abstract or try the inevrse.

#### Dataset Description
The dataset is Arxiv Paper Metadata from 2007 to 2017. 
Link: https://github.com/avenxu/arxiv_meta.git


#### Mile stone
| Time      |     Task  |   Status   |
| :--------: | :--------:| :------:     |
| field1      |   field2  |  field3      |



#### Work Log



#### Sources used
| Task       |     Link |  
| :-------- | :--------|
| A Neural Attention Model for Sentence Summarization    |   https://www.aclweb.org/anthology/D/D15/D15-1044.pdf | 
|Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation|https://www.aclweb.org/anthology/D14-1179 | 
|An architectrue of RNN seq2seq model |http://ceur-ws.org/Vol-1769/paper12.pdf |
|Earlier approach for text summarization(a bench mark using statical learning)|ftp://ftp.cse.buffalo.edu/users/azhang/disc/disc01/cd1/out/papers/sigir/p152-chuang.pdf |
| Overview of text generation with various models |  https://noon99jaki.github.io/publication/2017-text-gen.pdf <br>  https://arxiv.org/pdf/1711.09534.pdf <br> https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7944061 |
| Understanding convolution and dilated CNN on text | https://medium.com/@TalPerry/convolutional-methods-for-text-d5260fd5675f <br> https://arxiv.org/pdf/1509.01626.pdf|
| Understand implementation of Neural Machine Translation (seq2seq) | https://github.com/tensorflow/nmt  <br> https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html <br> https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf <br> https://ai.googleblog.com/2017/04/introducing-tf-seq2seq-open-source.html |
| Understand implementation of CharRNN | http://karpathy.github.io/2015/05/21/rnn-effectiveness/ <br> https://arxiv.org/pdf/1508.06615.pdf| 
| RNN for text genenration | https://arxiv.org/pdf/1308.0850.pdf <br> http://www.cs.utoronto.ca/~ilya/pubs/2011/LANG-RNN.pdf|
| Understand theory and implementation of Shakespeare CharRNN |https://blog.owulveryck.info/2017/10/29/about-recurrent-neural-network-shakespeare-and-go.html |
|(interesting) An architectrue combines CNN and RNN for news title summarization |https://web.stanford.edu/class/cs224n/reports/2760356.pdf |
| Graph architecture for seq2seq model  |  http://adventuresinmachinelearning.com/keras-lstm-tutorial/ |
|Could possibly be adapted Pointer-Generator Networks| https://arxiv.org/pdf/1704.04368.pdf |
|(interesting)Can be considered to genenrate abstracts with different strength for a subject|https://arxiv.org/pdf/1702.08139.pdf |



